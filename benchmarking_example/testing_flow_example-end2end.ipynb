{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8cf496",
   "metadata": {},
   "source": [
    "Implementing evaluation mertics on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c71fb90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai\n",
      "  Downloading openai-1.72.0-py3-none-any.whl (643 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.9/643.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (3.7.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.8/351.8 kB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m360.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.5.7)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m418.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m398.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, jiter, h11, distro, annotated-types, typing-inspection, pydantic-core, httpcore, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openai-1.72.0 pydantic-2.11.3 pydantic-core-2.33.1 typing-extensions-4.13.2 typing-inspection-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.30-py3-none-any.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.2/358.2 kB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.17)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m212.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Installing collected packages: zstandard, tenacity, packaging, orjson, jsonpatch, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed jsonpatch-1.33 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langsmith-0.3.30 orjson-3.10.16 packaging-24.2 requests-toolbelt-1.0.0 tenacity-9.1.2 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.3.101)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.1\n",
      "    Uninstalling safetensors-0.4.1:\n",
      "      Successfully uninstalled safetensors-0.4.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 sentence-transformers-4.0.2 tokenizers-0.21.1 transformers-4.51.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rdflib\n",
      "  Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib) (3.1.0)\n",
      "Installing collected packages: rdflib\n",
      "Successfully installed rdflib-7.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install pandas\n",
    "!pip install -U sentence-transformers\n",
    "!pip install rdflib\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8f0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 08:13:14.737911: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-11 08:13:14.846923: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-11 08:13:15.403336: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11 08:13:17.118494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca643a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the folder path\n",
    "# folder_path = \"/Users/rastegar-a/Documents/GitHub/i-adopt-llm-based-service/RDF-modelling-examples/Annotated_variables/\"\n",
    "# # Loop through all files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         csv_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "#         # Read the CSV file\n",
    "#         df = pd.read_csv(csv_path)\n",
    "        \n",
    "#         # Define JSON output path\n",
    "#         json_filename = filename.replace(\".csv\", \".json\")\n",
    "#         json_path = os.path.join(\"/Users/rastegar-a/Documents/GitHub/i-adopt-llm-based-service/benchmarking_example/\", json_filename)\n",
    "        \n",
    "#         # Convert to JSON\n",
    "#         df.to_json(json_path, orient=\"records\", lines=True)\n",
    "        \n",
    "#         print(f\"Converted {filename} to {json_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84703d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load from .env file in current directory\n",
    "load_dotenv()\n",
    "\n",
    "# # Get the token from the environment\n",
    "# hf_token = os.getenv(\"hugging_face_api_key\")\n",
    "\n",
    "# # Optional: check if token is loaded\n",
    "# if not hf_token:\n",
    "#     raise ValueError(\"HUGGINGFACE_HUB_TOKEN not found in .env\")\n",
    "\n",
    "# # Login to Hugging Face Hub\n",
    "# login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8305a94a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Initialize OpenRouter client (replace <OPENROUTER_API_KEY> with your actual API key).\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),    \n",
    ")\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.getenv(\"OPENAI_API_KEY\"),    \n",
    "# )\n",
    "\n",
    "# 2) Folder containing JSON files (each file has one ground-truth variable record)\n",
    "json_folder = \"data/\"\n",
    "# json_folder = \"/Users/rastegar-a/Documents/GitHub/i-adopt-llm-based-service/benchmarking_example/data/one_variable/\"\n",
    "\n",
    "# 3) Models you want to compare on OpenRouter or OpenAI\n",
    "# model_names = [\"deepseek/deepseek-v3-base:free\", \"google/gemini-2.5-pro-exp-03-25:free\"] # OpenRouter models\n",
    "# model_names = [\"deepseek/deepseek-v3-base:free\"] # OpenRouter models\n",
    "# model_names = [\"gpt-4o-mini\", \"gpt-4o\"]\n",
    "model_names = [\"meta-llama/llama-4-scout:free\", \"qwen/qwq-32b:free\"]\n",
    "\n",
    "\n",
    "# 4) Prompt template for asking the model to decompose the variable\n",
    "PROMPT_PATH = \"prompts/prompt_with_examples.txt\"\n",
    "with open(PROMPT_PATH,\"r\") as file:\n",
    "    template = file.read()\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"variable\", \"description\"]\n",
    ")\n",
    "\n",
    "# 5) LLM call helper using OpenRouter's chat endpoint\n",
    "def call_model_openrouter(model_name, user_prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 6) Embedding model (SentenceTransformer) for checking similarity\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embedding_similarity(text1, text2):\n",
    "    \"\"\"Compute cosine similarity between two pieces of text.\"\"\"\n",
    "    emb1 = embed_model.encode(text1, convert_to_tensor=True)\n",
    "    emb2 = embed_model.encode(text2, convert_to_tensor=True)\n",
    "    return util.cos_sim(emb1, emb2).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2962d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) We'll evaluate each of these keys with a threshold for correctness\n",
    "# ONTO_KEYS = [\"hasObjectOfInterest\", \"hasProperty\",  \"hasMatrix\", \"hasConstraint\", \"hasContext\"]\n",
    "ONTO_KEYS = [\"hasObjectOfInterest\", \"objectOfInterestURI\", \"hasProperty\", \"hasPropertyURI\", \"hasMatrix\", \"MatrixURI\", \"hasConstraint\", \"ConstraintURI\", \"constrain1\", \"hasContext\", \"ContextURI\"]\n",
    "THRESHOLD = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c88468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_confusion_for_field(gt_val, pred_val, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Correct Logic:\n",
    "      True Positive (TP):   GT not empty, pred not empty, similarity >= threshold\n",
    "      False Positive (FP):  GT empty, pred not empty\n",
    "      False Negative (FN):  GT not empty and (pred empty OR similarity < threshold)\n",
    "      True Negative (TN):   GT empty, pred empty\n",
    "    \"\"\"\n",
    "    # Strip leading/trailing whitespace\n",
    "    gt_val = gt_val.strip()\n",
    "    pred_val = pred_val.strip()\n",
    "    \n",
    "    # If ground truth is non-empty => label is \"present\".\n",
    "    if gt_val:\n",
    "        # Prediction non-empty => check similarity\n",
    "        if pred_val:\n",
    "            sim = embedding_similarity(gt_val, pred_val)\n",
    "            if sim >= threshold:\n",
    "                return (1, 0, 0, 0)  # TP\n",
    "            else:\n",
    "                return (0, 0, 1, 0)  # FN (prediction too dissimilar)\n",
    "        else:\n",
    "            # Prediction empty => definitely FN\n",
    "            return (0, 0, 1, 0)\n",
    "    \n",
    "    # If ground truth is empty => label is \"absent\".\n",
    "    else:\n",
    "        if pred_val:\n",
    "            # Predicted something when nothing was needed => FP\n",
    "            return (0, 1, 0, 0)\n",
    "        else:\n",
    "            # Both empty => TN\n",
    "            return (0, 0, 0, 1)\n",
    "\n",
    "    \n",
    "\n",
    "# Helper to compute precision, recall, f1 from confusion matrix totals\n",
    "def precision_recall_f1(tp, fp, fn, tn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099263ad-f1cb-40b2-90b9-8686b37d3d63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_component(graph, component):\n",
    "    assert component in [\"hasObjectOfInterest\",\"hasProperty\",\"hasMatrix\",\"hasConstraint\",\"hasContext\"]\n",
    "\n",
    "    q = '''\n",
    "        PREFIX iadopt: <https://w3id.org/iadopt/ont/>\n",
    "        \n",
    "        SELECT ?name\n",
    "        WHERE {\n",
    "            ?p rdf:type iadopt:Variable .\n",
    "        \n",
    "            ?p iadopt:{component} ?name .\n",
    "        }\n",
    "    '''\n",
    "    q = q.replace(\"{component}\",component)\n",
    "    \n",
    "    output = None\n",
    "    for r in g.query(q):\n",
    "        ## We should admit multiple hasMatrix, hasContraint, and hasContext. \n",
    "        #if component in [\"hasObjectOfInterest\",\"hasProperty\"]:\n",
    "        if component in [\"hasObjectOfInterest\",\"hasProperty\",\"hasMatrix\",\"hasConstraint\",\"hasContext\"]:\n",
    "            output = r[\"name\"].rsplit('/')[-1]\n",
    "            output = output.rsplit('#')[-1]\n",
    "            break\n",
    "        else:\n",
    "            if output is None:\n",
    "                output = []\n",
    "            output.append(r[\"name\"].replace(\"https://w3id.org/iadopt/ont/\",\"\"))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfd97d98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8) Main loop over JSON files\n",
    "all_rows = []  # We'll store row-based results to build a DF\n",
    "\n",
    "for file_name in os.listdir(json_folder):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(json_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                data = json.loads(line)\n",
    "\n",
    "                variable_text = data.get(\"Variable\", \"\")\n",
    "                description_text = data.get(\"description\", \"\")\n",
    "                ground_truth = {k: data.get(k, \"\") for k in ONTO_KEYS}\n",
    "                prompt_text = prompt_template.format(\n",
    "                    variable=variable_text,\n",
    "                    description=description_text\n",
    "                )\n",
    "\n",
    "                # For each model, get predictions and compute confusion matrix\n",
    "                for model_name in model_names:\n",
    "                    llm_output = call_model_openrouter(model_name, prompt_text)\n",
    "\n",
    "                    pattern = r\"```(\\w*)\\n(.*?)\\n```\"\n",
    "                    matches = re.findall(pattern, llm_output, re.DOTALL)\n",
    "                    for match in matches:\n",
    "                        language = match[0]\n",
    "                        context = match[1]\n",
    "                        break\n",
    "\n",
    "                    with open(\"output.ttl\",\"w\") as file:\n",
    "                        if \"@prefix : <https://w3id.org/iadopt/ont/> .\" not in context:\n",
    "                            context = \"@prefix : <https://w3id.org/iadopt/ont/> .\\n\" + context\n",
    "                        file.write(context)\n",
    "                    \n",
    "                    g = Graph()\n",
    "                    g.parse(\"output.ttl\",format=\"turtle\") # Maybe if the rdf file cannot be parse retry\n",
    "                    \n",
    "                    predicted_json = {\"hasObjectOfInterest\":None,\"hasProperty\":None, \"hasMatrix\": None, \"hasConstraint\":None, \"hasContext\": None}\n",
    "                    \n",
    "                    for elem in predicted_json:\n",
    "                        predicted_json[elem]=extract_component(g,elem)\n",
    "\n",
    "                    # Accumulate confusion counts across all keys\n",
    "                    total_tp = total_fp = total_fn = total_tn = 0\n",
    "                    for key in ONTO_KEYS:\n",
    "                        gt_val = ground_truth.get(key, \"\") or \"\"\n",
    "                        pred_val = predicted_json.get(key, \"\") or \"\"\n",
    "                        tp, fp, fn, tn = compute_confusion_for_field(gt_val, pred_val)\n",
    "                        total_tp += tp\n",
    "                        total_fp += fp\n",
    "                        total_fn += fn\n",
    "                        total_tn += tn\n",
    "\n",
    "                    prec, rec, f1 = precision_recall_f1(total_tp, total_fp, total_fn, total_tn)\n",
    "\n",
    "                    # Store everything in all_rows, including ground truth & the predicted JSON\n",
    "                    row_dict = {\n",
    "                        \"File\": file_name,\n",
    "                        \"Variable\": variable_text,\n",
    "                        \"Model\": model_name,\n",
    "                        \"TP\": total_tp,\n",
    "                        \"FP\": total_fp,\n",
    "                        \"FN\": total_fn,\n",
    "                        \"TN\": total_tn,\n",
    "                        \"Precision\": round(prec, 3),\n",
    "                        \"Recall\": round(rec, 3),\n",
    "                        \"F1\": round(f1, 3),\n",
    "                        # Store ground truth & predicted as strings for easy reference\n",
    "                        \"GroundTruth\": json.dumps(ground_truth),\n",
    "                        \"LLMOutput\": json.dumps(predicted_json)\n",
    "                    }\n",
    "                    all_rows.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dafb77c6-4735-411f-beae-605001bf49d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results DataFrame ===\n",
      "\n",
      "          File                                           Variable  \\\n",
      "0    var3.json                                        Cloud cover   \n",
      "1    var3.json                                        Cloud cover   \n",
      "2    var1.json                 Electron density in the solar wind   \n",
      "3    var1.json                 Electron density in the solar wind   \n",
      "4    var5.json  Atmosphere_optical_thickness_due_to_particulat...   \n",
      "5    var5.json  Atmosphere_optical_thickness_due_to_particulat...   \n",
      "6   var17.json  Docosahexaenoic acid content per dry weight (D...   \n",
      "7   var17.json  Docosahexaenoic acid content per dry weight (D...   \n",
      "8    var4.json                 Atmospheric boundary layer heights   \n",
      "9    var4.json                 Atmospheric boundary layer heights   \n",
      "10   var2.json                      Air daily maximum temperature   \n",
      "11   var2.json                      Air daily maximum temperature   \n",
      "\n",
      "                            Model  TP  FP  FN  TN  Precision  Recall     F1  \\\n",
      "0   meta-llama/llama-4-scout:free   0   0   6   5        0.0   0.000  0.000   \n",
      "1               qwen/qwq-32b:free   0   0   6   5        0.0   0.000  0.000   \n",
      "2   meta-llama/llama-4-scout:free   2   0   4   5        1.0   0.333  0.500   \n",
      "3               qwen/qwq-32b:free   3   0   3   5        1.0   0.500  0.667   \n",
      "4   meta-llama/llama-4-scout:free   1   0   7   3        1.0   0.125  0.222   \n",
      "5               qwen/qwq-32b:free   1   1   7   2        0.5   0.125  0.200   \n",
      "6   meta-llama/llama-4-scout:free   1   1   5   4        0.5   0.167  0.250   \n",
      "7               qwen/qwq-32b:free   1   1   5   4        0.5   0.167  0.250   \n",
      "8   meta-llama/llama-4-scout:free   0   0   6   5        0.0   0.000  0.000   \n",
      "9               qwen/qwq-32b:free   0   0   6   5        0.0   0.000  0.000   \n",
      "10  meta-llama/llama-4-scout:free   2   0   7   2        1.0   0.222  0.364   \n",
      "11              qwen/qwq-32b:free   2   0   7   2        1.0   0.222  0.364   \n",
      "\n",
      "                                          GroundTruth  \\\n",
      "0   {\"hasObjectOfInterest\": \"sky\", \"objectOfIntere...   \n",
      "1   {\"hasObjectOfInterest\": \"sky\", \"objectOfIntere...   \n",
      "2   {\"hasObjectOfInterest\": \"electrons\", \"objectOf...   \n",
      "3   {\"hasObjectOfInterest\": \"electrons\", \"objectOf...   \n",
      "4   {\"hasObjectOfInterest\": \"Aerosol\", \"objectOfIn...   \n",
      "5   {\"hasObjectOfInterest\": \"Aerosol\", \"objectOfIn...   \n",
      "6   {\"hasObjectOfInterest\": \"docosahexaenoic acid\"...   \n",
      "7   {\"hasObjectOfInterest\": \"docosahexaenoic acid\"...   \n",
      "8   {\"hasObjectOfInterest\": \"valley floor\", \"objec...   \n",
      "9   {\"hasObjectOfInterest\": \"valley floor\", \"objec...   \n",
      "10  {\"hasObjectOfInterest\": \"air\", \"objectOfIntere...   \n",
      "11  {\"hasObjectOfInterest\": \"air\", \"objectOfIntere...   \n",
      "\n",
      "                                            LLMOutput  \n",
      "0   {\"hasObjectOfInterest\": \"clouds\", \"hasProperty...  \n",
      "1   {\"hasObjectOfInterest\": \"clouds\", \"hasProperty...  \n",
      "2   {\"hasObjectOfInterest\": \"electrons\", \"hasPrope...  \n",
      "3   {\"hasObjectOfInterest\": \"electrons\", \"hasPrope...  \n",
      "4   {\"hasObjectOfInterest\": \"particulate_organic_m...  \n",
      "5   {\"hasObjectOfInterest\": \"particulate_organic_m...  \n",
      "6   {\"hasObjectOfInterest\": \"docosahexaenoic_acid\"...  \n",
      "7   {\"hasObjectOfInterest\": \"docosahexaenoic_acid\"...  \n",
      "8   {\"hasObjectOfInterest\": \"atmospheric_boundary_...  \n",
      "9   {\"hasObjectOfInterest\": \"atmospheric_boundary_...  \n",
      "10  {\"hasObjectOfInterest\": \"air\", \"hasProperty\": ...  \n",
      "11  {\"hasObjectOfInterest\": \"air\", \"hasProperty\": ...  \n",
      "\n",
      "=== Summary (Grouped by File, Model) ===\n",
      "\n",
      "          File                          Model  Precision  Recall     F1\n",
      "0    var1.json  meta-llama/llama-4-scout:free        1.0   0.333  0.500\n",
      "1    var1.json              qwen/qwq-32b:free        1.0   0.500  0.667\n",
      "2   var17.json  meta-llama/llama-4-scout:free        0.5   0.167  0.250\n",
      "3   var17.json              qwen/qwq-32b:free        0.5   0.167  0.250\n",
      "4    var2.json  meta-llama/llama-4-scout:free        1.0   0.222  0.364\n",
      "5    var2.json              qwen/qwq-32b:free        1.0   0.222  0.364\n",
      "6    var3.json  meta-llama/llama-4-scout:free        0.0   0.000  0.000\n",
      "7    var3.json              qwen/qwq-32b:free        0.0   0.000  0.000\n",
      "8    var4.json  meta-llama/llama-4-scout:free        0.0   0.000  0.000\n",
      "9    var4.json              qwen/qwq-32b:free        0.0   0.000  0.000\n",
      "10   var5.json  meta-llama/llama-4-scout:free        1.0   0.125  0.222\n",
      "11   var5.json              qwen/qwq-32b:free        0.5   0.125  0.200\n"
     ]
    }
   ],
   "source": [
    "# 9) Create a DataFrame with aggregated results\n",
    "df_results = pd.DataFrame(all_rows)\n",
    "print(\"\\n=== Final Results DataFrame ===\\n\")\n",
    "print(df_results)\n",
    "\n",
    "# Group by [File, Model] to see average metrics if multiple lines in one file\n",
    "summary = df_results.groupby([\"File\", \"Model\"]).agg({\n",
    "    \"Precision\": \"mean\",\n",
    "    \"Recall\": \"mean\",\n",
    "    \"F1\": \"mean\"\n",
    "}).reset_index()\n",
    "summary = summary.round(3)\n",
    "\n",
    "print(\"\\n=== Summary (Grouped by File, Model) ===\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e9d3e7-f5a3-4c89-8db9-5a3aade282c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ground Truth vs. LLM Output Details ===\n",
      "\n",
      "File: var3.json\n",
      "Model: meta-llama/llama-4-scout:free\n",
      "Variable: Cloud cover\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"sky\", \"objectOfInterestURI\": \"https://example.org/ex/sky\", \"hasProperty\": \"cloudiness\", \"hasPropertyURI\": \"https://example.org/cloudiness\", \"hasMatrix\": \"study site\", \"MatrixURI\": \"http://purl.bioontology.org/ontology/LNC/MTHU054795\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"clouds\", \"hasProperty\": \"cloud_cover\", \"hasMatrix\": null, \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var3.json\n",
      "Model: qwen/qwq-32b:free\n",
      "Variable: Cloud cover\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"sky\", \"objectOfInterestURI\": \"https://example.org/ex/sky\", \"hasProperty\": \"cloudiness\", \"hasPropertyURI\": \"https://example.org/cloudiness\", \"hasMatrix\": \"study site\", \"MatrixURI\": \"http://purl.bioontology.org/ontology/LNC/MTHU054795\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"clouds\", \"hasProperty\": \"cloudCover\", \"hasMatrix\": \"study_site\", \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var1.json\n",
      "Model: meta-llama/llama-4-scout:free\n",
      "Variable: Electron density in the solar wind\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"electrons\", \"objectOfInterestURI\": \"http://purl.bioontology.org/ontology/LNC/LA3953-2\", \"hasProperty\": \"density\", \"hasPropertyURI\": \"http://www.ontology-of-units-of-measure.org/resource/om-2/Density\", \"hasMatrix\": \"Solar Wind\", \"MatrixURI\": \"http://sweetontology.net/phenHelio/SolarWind\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"electrons\", \"hasProperty\": \"density\", \"hasMatrix\": null, \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var1.json\n",
      "Model: qwen/qwq-32b:free\n",
      "Variable: Electron density in the solar wind\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"electrons\", \"objectOfInterestURI\": \"http://purl.bioontology.org/ontology/LNC/LA3953-2\", \"hasProperty\": \"density\", \"hasPropertyURI\": \"http://www.ontology-of-units-of-measure.org/resource/om-2/Density\", \"hasMatrix\": \"Solar Wind\", \"MatrixURI\": \"http://sweetontology.net/phenHelio/SolarWind\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"electrons\", \"hasProperty\": \"density\", \"hasMatrix\": \"solar_wind\", \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var5.json\n",
      "Model: meta-llama/llama-4-scout:free\n",
      "Variable: Atmosphere_optical_thickness_due_to_particulate_organic_matter_ambient_aerosol\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"Aerosol\", \"objectOfInterestURI\": \"http://purl.bioontology.org/ontology/SNOMEDCT/64242006\", \"hasProperty\": \"Optical Thickness\", \"hasPropertyURI\": \"http://sweetontology.net/propSpaceThickness/OpticalThickness\", \"hasMatrix\": \"Atmosphere\", \"MatrixURI\": \"http://purl.bioontology.org/ontology/SNOMEDCT/304607008\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": \"Path of Radiation\", \"ContextURI\": \"http://www.example.org/path_of_radition\"}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"particulate_organic_matter\", \"hasProperty\": \"optical_thickness\", \"hasMatrix\": null, \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var5.json\n",
      "Model: qwen/qwq-32b:free\n",
      "Variable: Atmosphere_optical_thickness_due_to_particulate_organic_matter_ambient_aerosol\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"Aerosol\", \"objectOfInterestURI\": \"http://purl.bioontology.org/ontology/SNOMEDCT/64242006\", \"hasProperty\": \"Optical Thickness\", \"hasPropertyURI\": \"http://sweetontology.net/propSpaceThickness/OpticalThickness\", \"hasMatrix\": \"Atmosphere\", \"MatrixURI\": \"http://purl.bioontology.org/ontology/SNOMEDCT/304607008\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": \"Path of Radiation\", \"ContextURI\": \"http://www.example.org/path_of_radition\"}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"particulate_organic_matter\", \"hasProperty\": \"optical_thickness\", \"hasMatrix\": \"aerosol\", \"hasConstraint\": \"ambient\", \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var17.json\n",
      "Model: meta-llama/llama-4-scout:free\n",
      "Variable: Docosahexaenoic acid content per dry weight (DHA content/ C22:6 n-3 content)\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"docosahexaenoic acid\", \"objectOfInterestURI\": \"http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C68345\", \"hasProperty\": \"relative dry weight\", \"hasPropertyURI\": \"http://purl.obolibrary.org/obo/TO_0000633\", \"hasMatrix\": \"individual\", \"MatrixURI\": \"http://www.ebi.ac.uk/efo/EFO_0000542\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"docosahexaenoic_acid\", \"hasProperty\": \"content\", \"hasMatrix\": null, \"hasConstraint\": \"dry_weight\", \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var17.json\n",
      "Model: qwen/qwq-32b:free\n",
      "Variable: Docosahexaenoic acid content per dry weight (DHA content/ C22:6 n-3 content)\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"docosahexaenoic acid\", \"objectOfInterestURI\": \"http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C68345\", \"hasProperty\": \"relative dry weight\", \"hasPropertyURI\": \"http://purl.obolibrary.org/obo/TO_0000633\", \"hasMatrix\": \"individual\", \"MatrixURI\": \"http://www.ebi.ac.uk/efo/EFO_0000542\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"docosahexaenoic_acid\", \"hasProperty\": \"content\", \"hasMatrix\": \"individual_organism\", \"hasConstraint\": \"dry_weight\", \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var4.json\n",
      "Model: meta-llama/llama-4-scout:free\n",
      "Variable: Atmospheric boundary layer heights\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"valley floor\", \"objectOfInterestURI\": \"https://example.org/valley_floor\", \"hasProperty\": \"height-range\", \"hasPropertyURI\": \"http://sweetontology.net/propSpaceHeight/HeightRange\", \"hasMatrix\": \"Boundary Layer/Free Troposphere\", \"MatrixURI\": \"http://sweetontology.net/realmAtmo/FreeTroposphere\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"atmospheric_boundary_layer\", \"hasProperty\": \"height\", \"hasMatrix\": null, \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var4.json\n",
      "Model: qwen/qwq-32b:free\n",
      "Variable: Atmospheric boundary layer heights\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"valley floor\", \"objectOfInterestURI\": \"https://example.org/valley_floor\", \"hasProperty\": \"height-range\", \"hasPropertyURI\": \"http://sweetontology.net/propSpaceHeight/HeightRange\", \"hasMatrix\": \"Boundary Layer/Free Troposphere\", \"MatrixURI\": \"http://sweetontology.net/realmAtmo/FreeTroposphere\", \"hasConstraint\": null, \"ConstraintURI\": null, \"constrain1\": null, \"hasContext\": null, \"ContextURI\": null}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"atmospheric_boundary_layer\", \"hasProperty\": \"height\", \"hasMatrix\": \"valley_floor\", \"hasConstraint\": null, \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var2.json\n",
      "Model: meta-llama/llama-4-scout:free\n",
      "Variable: Air daily maximum temperature\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"air\", \"objectOfInterestURI\": \"http://purl.obolibrary.org/obo/ENVO_00002005\", \"hasProperty\": \"Temperature \", \"hasPropertyURI\": \"http://ecoinformatics.org/oboe/oboe.1.2/oboe-characteristics.owl#Temperature\", \"hasMatrix\": null, \"MatrixURI\": null, \"hasConstraint\": \"height\", \"ConstraintURI\": \"http://www.ontology-of-units-of-measure.org/resource/om-2/Height\", \"constrain1\": \"air\", \"hasContext\": \"daily maximum air temperature\", \"ContextURI\": \"http://purl.obolibrary.org/obo/ECOSIM_TAMX\"}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"air\", \"hasProperty\": \"temperature\", \"hasMatrix\": null, \"hasConstraint\": \"daily_maximum\", \"hasContext\": null}\n",
      "--------------------------------------------------\n",
      "File: var2.json\n",
      "Model: qwen/qwq-32b:free\n",
      "Variable: Air daily maximum temperature\n",
      "Ground Truth: {\"hasObjectOfInterest\": \"air\", \"objectOfInterestURI\": \"http://purl.obolibrary.org/obo/ENVO_00002005\", \"hasProperty\": \"Temperature \", \"hasPropertyURI\": \"http://ecoinformatics.org/oboe/oboe.1.2/oboe-characteristics.owl#Temperature\", \"hasMatrix\": null, \"MatrixURI\": null, \"hasConstraint\": \"height\", \"ConstraintURI\": \"http://www.ontology-of-units-of-measure.org/resource/om-2/Height\", \"constrain1\": \"air\", \"hasContext\": \"daily maximum air temperature\", \"ContextURI\": \"http://purl.obolibrary.org/obo/ECOSIM_TAMX\"}\n",
      "LLM Output: {\"hasObjectOfInterest\": \"air\", \"hasProperty\": \"temperature\", \"hasMatrix\": null, \"hasConstraint\": \"1_7_meter_height\", \"hasContext\": null}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 10) Finally, show ground truth and LLM outputs after the summary\n",
    "print(\"\\n=== Ground Truth vs. LLM Output Details ===\\n\")\n",
    "for idx, row in df_results.iterrows():\n",
    "    print(\"File:\", row[\"File\"])\n",
    "    print(\"Model:\", row[\"Model\"])\n",
    "    print(\"Variable:\", row[\"Variable\"])\n",
    "    print(\"Ground Truth:\", row[\"GroundTruth\"])\n",
    "    print(\"LLM Output:\", row[\"LLMOutput\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e728c7-18d6-4584-8c5e-45ad6eb5c3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
